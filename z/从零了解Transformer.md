---
tags:
  - tech/ai/ml
  - type/concept
  - status/seed
description: Transformer架构原理与注意力机制入门
created: 2025-01-01T00:00:00
updated: 2025-12-07T21:16:36
---

> [!info] **上级索引**
> [[AI MOC]] | [[深度学习 MOC]]

---


# 从零了解Transformer 

## 基础概念

### 函数

*生活中关于函数的难点不是 知道函数以及参数后得到结果，而在于获取一个函数，有了问题后就能得到结果。*

1+1 ->  F(n) -> ?
怎么做蛋炒饭 -> F(n) -> ?
帮我写一篇800字文章 -> F(n) -> ?
图片中的是人吗 -> F(n) -> ?

transformer 就是一个函数，能通过输入得到一个结果。

### 联结主义

显然对于简单的问题，我们能够一眼看出这个 F(n) 是什么，但是对于复杂的问题，我们只能靠猜测，去找规律。
去猜出这个函数！！！

### 激活函数

f(x) = g(w1x1+w2x2+b)

通过向量等等的数学方法，把输入输出都变为向量，搞成能计算的形式。
然后需要的函数就是一个接受输入参数（x1、x2.。。）通过调整参数（w1、w2）来实现输入的函数。

### 拟合

一个好的 ai 模型就是一个好的参数，好的架构，好的函数，能够让输入经过变换后得到想要的输出，也就是 函数的曲线和 关键坐标点拟合。

### 训练

训练就是通过调整参数，来让函数更拟合。

### 潜空间

每个词都有一个对应的向量。
好像是通过 ai 自己生成的。
用于计算

## 使用指南

## 实战经验

## 经验总结

**既然现在的ai训练是通过已知的输入输出来训练的，那他是不是永远不可能有智能？**

## 信息参考

[从函数到神经网络【白话DeepSeek01】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1uGA3eLEeu?vd_source=15c229538881316b8c6a43f997de056f&spm_id_from=333.788.videopod.sections)
